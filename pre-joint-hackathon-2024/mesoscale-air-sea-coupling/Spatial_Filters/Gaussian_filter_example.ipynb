{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3315f5-4cb7-41da-9e8a-6b1f87c59249",
   "metadata": {},
   "source": [
    "## Gaussian filter from GCM filters\n",
    "- Use a variable 30*R (R=Rossby radius) spatial filter\n",
    "- Perform filtering on 0.25deg data as GCM filters can only work on gridded data. Most functions provided by Matthias Aengenheyster and much of this example is taken from https://github.com/eerie-project/EERIE_hackathon_2023/blob/main/RESULTS/eddy_composites-short.ipynb\n",
    "- Intake catalog of EERIE data done by Fabian Wachsmann\n",
    "\n",
    "Feb 2024, Matthias Aengenheyster (ECMWF) and Dian Putrasahan (MPIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1d723c-4e28-499a-914f-d741f3150e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some function from Matthias to be copied over. \n",
    "# cp -p /home/b/b382473/*.py /home/m/m300466/pyfuncs/MA\n",
    "# cp -p /home/b/b382473/LR_filtered_1degree_r1440x721.nc /work/mh0256/m300466/ifsfesomgrids/RossbyRadius_filtered_1degree_r1440x721.nc\n",
    "# cp -p /work/bm1344/a270228/EERIE_Hackathon/IFS-FESOM_CONTROL-1950/tco1279-NG5/1950/FESOM/025/daily/ssh_1950_19500101-19500131_025.nc /work/mh0256/m300466/ifsfesomgrids/ssh_1950_IFS25.nc\n",
    "\n",
    "import gcm_filters\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import intake\n",
    "import os,sys\n",
    "from datetime import datetime\n",
    "from dask.diagnostics import ProgressBar\n",
    "import time as T\n",
    "\n",
    "sys.path.insert(0, r'/home/m/m300466/pyfuncs/MA')\n",
    "import geostats as gs\n",
    "import filtering as fl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47990566-fbec-4c47-b3ac-c8fe29a1d77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def ifs_to_latlon(ds):\n",
    "#     '''\n",
    "#     Uses the latitude-longitude information encoded in the regular grid IFS output to reconstructed the regular grid\n",
    "#     '''\n",
    "#     return ds.rename({'value':'latlon'}).set_index(latlon=(\"lat\",\"lon\")).unstack(\"latlon\")\n",
    "\n",
    "# def ifs_fix_time_for_monthly_data(ds):\n",
    "#     '''\n",
    "#     Monthly mean data has the time axis encoded incorrectly.\n",
    "#     This function shifts it back by one day\n",
    "#     '''\n",
    "#     return ds.assign_coords(time=ds['time']- pd.Timedelta('1D'))\n",
    "\n",
    "def get_area(da,mask=False):\n",
    "    print('Computing grid-box area')\n",
    "    import iris.analysis as ia\n",
    "    if 'time' in da.dims:\n",
    "        da = da.isel(time=0).drop('time')\n",
    "    d = da.to_iris()\n",
    "    d.coord('longitude').guess_bounds()\n",
    "    d.coord('latitude').guess_bounds()\n",
    "\n",
    "    area_weights = ia.cartography.area_weights(d)\n",
    "    area = xr.ones_like(da) * area_weights\n",
    "    if mask:\n",
    "        area = area.where(~np.isnan(da))\n",
    "    area = area.rename('area').load()\n",
    "    area.attrs['long_name'] = 'grid_box_area'\n",
    "    area.attrs['units'] = 'm^2'\n",
    "    return area\n",
    "\n",
    "def print_var(ds,filt=None):\n",
    "    '''\n",
    "    Print variables (varname,name) in Dataset. \n",
    "    If <filt> is provided, print only those where <filt> is present in the <name> attribute (ds[shortname].attrs['name'])\n",
    "\n",
    "    Usage: print_var(ds,'wind')\n",
    "    Output: printout of variables in ds whose name (long name in attributes, not short name to access) contains 'wind', e.g.\n",
    "        10si  :   10 metre wind speed\n",
    "        10u  :   10 metre U wind component\n",
    "        10v  :   10 metre V wind component\n",
    "    '''\n",
    "    if filt:\n",
    "        [print('%10s  :   %s' % (d,ds[d].attrs['name'])) for d in ds if filt.lower() in ds[d].attrs['name'].lower()]\n",
    "    else:\n",
    "        [print('%10s  :   %s' % (d,ds[d].attrs['name'])) for d in ds]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f60b18-f2f5-4eef-a131-a099d538b0ad",
   "metadata": {},
   "source": [
    "## Open and inspect catalogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e38304-f006-4fa6-bcd8-55089a5984d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daily', 'monthly']\n"
     ]
    }
   ],
   "source": [
    "cat = intake.open_catalog(\"https://raw.githubusercontent.com/eerie-project/intake_catalogues/main/eerie.yaml\")\n",
    "model = 'ifs-fesom2-sr'\n",
    "# model = 'icon-esm-er'\n",
    "expid = 'eerie-control-1950'\n",
    "gridspec = 'gr025'\n",
    "realm='ocean'\n",
    "# realm='atmos'\n",
    "cat_regrid = cat['dkrz.disk.model-output'][model][expid][realm][gridspec]\n",
    "print(list(cat_regrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d845ab-d0ad-4f7d-8aa2-b5a21796028e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds1 = cat_regrid['daily'].to_dask()\n",
    "# ds1 = cat_regrid['daily_2d'].to_dask()\n",
    "# ds1 = cat_regrid['2d_daily_mean'].to_dask()\n",
    "\n",
    "timesel = dict(time=slice('1950-01-01','1950-12-31'))\n",
    "ds_subset1 = ds1.sel(timesel)\n",
    "# ds_subset1 = ds1\n",
    "datearr = np.array([pd.Timestamp(t).to_pydatetime() for t in ds_subset1.time.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473f259-e332-4d54-850d-0d4a36583fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "varname='sst'\n",
    "print('High pass filter daily '+varname+' for year='+str(datearr[0].year)+'-'+str(datearr[-1].year))\n",
    "wavelength='30R'\n",
    "bnds = dict(lat=slice(-85,85))\n",
    "\n",
    "# Directories\n",
    "scratch = '/scratch/m/m300466/'\n",
    "datadir = scratch+expid+'/'+gridspec+'/'\n",
    "if not os.path.exists(datadir+'/'+model):\n",
    "    os.makedirs(datadir+'/'+model)\n",
    "    \n",
    "vardir=datadir+model+'/'+varname\n",
    "filtdir=vardir+'/Gaussian'\n",
    "smdatadir=filtdir+'/sm'+wavelength\n",
    "if not os.path.exists(vardir):\n",
    "        os.makedirs(vardir)\n",
    "if not os.path.exists(filtdir):\n",
    "        os.makedirs(filtdir)\n",
    "if not os.path.exists(smdatadir):\n",
    "        os.makedirs(smdatadir)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f394f-d249-4747-920d-98f2307d4bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_da_field(ds, varname, bnds):\n",
    "    ds = ds[[varname]].sel(bnds)\n",
    "    # ds = fl.define_grid_metrics(ds)\n",
    "    \n",
    "    # wetmask_gl = xr.where(np.isnan(ds[varname].isel(time=0)),0,1).load()\n",
    "    wetmask_gl = xr.where(np.isnan(ds[varname]),0,1).load()\n",
    "    # Set mask to zero at southern (irrelevant - Antarctica) and northern (Arctic) boundary to prevent \"outflow\" of information\n",
    "    wetmask_gl[0,:] = 0\n",
    "    wetmask_gl[-1,:] = 0\n",
    "    wet_mask = wetmask_gl.drop('time').chunk({'lat': len(ds.lat), 'lon': len(ds.lon)})  # 1 chunk\n",
    "\n",
    "    da_field = ds[varname].where(wet_mask)\n",
    "    da_field = da_field.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)})  # 1 chunk\n",
    "    return da_field.to_dataset()\n",
    "    \n",
    "def setup_filter(ds, varname, bnds):\n",
    "    '''\n",
    "    Setup filter based on <varname> in ds, limited to geographical bnds\n",
    "    Using a Gaussian filter with varying spatial scales (factor * Rossby radius)\n",
    "    '''\n",
    "    ds = ds[[varname]].sel(bnds)\n",
    "    ds = fl.define_grid_metrics(ds)\n",
    "\n",
    "    dxw = ds.dxc.rename({'lon_g':'lon'}).assign_coords(lon=ds.lon).rename('dxw')#.sel(lat=slice(-60,60))\n",
    "    dyw = ds.dyg.rename({'lon_g':'lon'}).assign_coords(lon=ds.lon).rename('dyw')#.sel(lat=slice(-60,60))\n",
    "    dxs = ds.dxg.rename({'lat_g':'lat'}).assign_coords(lat=ds.lat).rename('dxs')#.sel(lat=slice(-60,60))\n",
    "    dys = ds.dyc.rename({'lat_g':'lat'}).assign_coords(lat=ds.lat).rename('dys')#.sel(lat=slice(-60,60))\n",
    "    # grid spacings, between cell centers (_c) and cell boundaries (_g), based on the MITgcm terminology\n",
    "    # dxw = x-spacing centered at western cell edge\n",
    "    # dyw = y-spacing centered at western cell edge\n",
    "    # dxs = x-spacing centered at southern cell edge\n",
    "    # dys = y-spacing centered at southern cell edge\n",
    "\n",
    "    dx_min = min(\n",
    "        dxw.min(),\n",
    "        dyw.min(),\n",
    "        dxs.min(),\n",
    "        dys.min()).values\n",
    "    \n",
    "    area = gs.get_area(ds[varname])\n",
    "\n",
    "    wetmask_gl = xr.where(np.isnan(ds[varname].isel(time=0)),0,1).load()\n",
    "    # wetmask_gl = xr.where(np.isnan(ds[varname]),0,1).load()\n",
    "    # Set mask to zero at southern (irrelevant - Antarctica) and northern (Arctic) boundary to prevent \"outflow\" of information\n",
    "    wetmask_gl[0,:] = 0\n",
    "    wetmask_gl[-1,:] = 0\n",
    "    \n",
    "    wet_mask = wetmask_gl.drop('time').chunk({'lat': len(ds.lat), 'lon': len(ds.lon)})  # 1 chunk\n",
    "    area = area.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)})  # 1 chunk\n",
    "    dxw = dxw.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)}) # 1 chunk\n",
    "    dyw = dyw.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)}) # 1 chunk\n",
    "    dxs = dxs.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)}) # 1 chunk\n",
    "    dys = dys.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)}) # 1 chunk\n",
    "\n",
    "    # da_field = ds[varname].where(wet_mask)\n",
    "    # da_field = da_field.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)})  # 1 chunk\n",
    "\n",
    "    # https://gcm-filters.readthedocs.io/en/latest/examples/example_filter_types.html#preparing-the-grid-input-variables  \n",
    "    # ======================\n",
    "    # ROSSBY-RADIUS BASED FILTERING\n",
    "    # ======================\n",
    "    LR_gl = xr.open_dataarray('/home/b/b382473/LR_filtered_1degree_r1440x721.nc').sel(bnds)\n",
    "    #is the Rossby radius regridded to the 1/4 degree grid from PHC3.0 climatology\n",
    "    \n",
    "    # factor 30, (300-1500 km)\n",
    "    LRfactor = 30\n",
    "    LRmax = 1500\n",
    "\n",
    "    # small: factor 5 (50-250 km)\n",
    "    # LRfactor = 5\n",
    "    # LRmax = 250\n",
    "    # large: factor 90 (900-4500 km)\n",
    "    # LRfactor = 90\n",
    "    # LRmax = 4500\n",
    "    # medium: factor 12 (120-600 km)\n",
    "    # LRfactor = 12\n",
    "    # LRmax = 600\n",
    "\n",
    "    LRmin = 10 # set minimum length to 10\n",
    "    LR_min_max30_gl = xr.where(LR_gl<LRmin,LRmin,LR_gl)\n",
    "    LR_min_max30_gl = xr.where(LR_min_max30_gl*LRfactor>LRmax,LRmax,LR_min_max30_gl*LRfactor) # 30 x rossby radius, capped at 1500 km\n",
    "    #LRfactor, LRmax, LRmin: multiple of Rossby radius, and maximum/minimum cutoffs\n",
    "    #NB: minimum is done before the  factor, but the maximum is done after\n",
    "    # Largest range of LR_min_max30_gl = (LRfactor * LRmin , LRmax) \n",
    "\n",
    "    # L_max = LR.max().values\n",
    "    L_max_gl = LR_min_max30_gl.max().values\n",
    "    filter_scale = L_max_gl\n",
    "    kappa_min_max30_gl = LR_min_max30_gl**2 / L_max_gl**2\n",
    "    kappa_min_max30_gl = kappa_min_max30_gl.chunk({'lat': len(ds.lat), 'lon': len(ds.lon)}) # 1 chunk\n",
    "\n",
    "    kappa_w = kappa_min_max30_gl.copy()\n",
    "    kappa_s = kappa_min_max30_gl.copy()\n",
    "    # kappa_w and kappa_s are the diffusivity in x and y, that needs a lengthscale for scaling.\n",
    "    # For a constant lengthscale, they are both =1, and one can change them separately to have different filter lengths per direction\n",
    "    # We should keep kappa to between 0 and 1. \n",
    "    \n",
    "    \n",
    "    specs = dict(\n",
    "        filter_scale=filter_scale * 1e3, #from km to m\n",
    "        dx_min=dx_min,\n",
    "        filter_shape=gcm_filters.FilterShape.GAUSSIAN,\n",
    "        grid_type=gcm_filters.GridType.IRREGULAR_WITH_LAND,\n",
    "    )\n",
    "\n",
    "    filter_cpu = gcm_filters.Filter(grid_vars={\n",
    "        'area': area,\n",
    "        'wet_mask': wet_mask,\n",
    "        'dxw': dxw,\n",
    "        'dyw': dyw,\n",
    "        'dxs': dxs,\n",
    "        'dys': dys,\n",
    "        'kappa_w': kappa_w,\n",
    "        'kappa_s': kappa_s,\n",
    "        }, **specs)\n",
    "    print('Filter: ',filter_cpu)\n",
    "    \n",
    "    return filter_cpu\n",
    "\n",
    "def filter_data(filter_obj, da_field, load=False):\n",
    "    fields_filtered = filter_obj.apply(\n",
    "        da_field, dims=['lat', 'lon'])\n",
    "        # ds, dims=['lon'])\n",
    "    if load:\n",
    "        with ProgressBar():\n",
    "            fields_filtered.load()\n",
    "    return fields_filtered\n",
    "\n",
    "def gaussian_gcm_filter(ds_subset1,varname,bnds,datearr,tt,smdatadir,wavelength,filter_cpu): \n",
    "    date = datearr[tt]\n",
    "    # Load data from xarray into netcdf4 type\n",
    "    ds_subset = ds_subset1[[varname]].sel(bnds).isel(time=tt)\n",
    "    # da_ssh.time.encoding.pop(\"_FillValue\",None)\n",
    "    da_field=get_da_field(ds_subset, varname, bnds)\n",
    "\n",
    "    print('High pass filter of '+varname+' for '+date.strftime('%Y%m%d'))\n",
    "    fields_filtered=filter_data(filter_cpu, da_field, load=False)\n",
    "\n",
    "    if varname=='to' or varname=='so' or varname=='rho':\n",
    "        zidx=1\n",
    "        foutname=smdatadir+'/'+varname+'_'+str(zidx)+'_'+date.strftime('%Y%m%d')+'_sm'+wavelength+'.nc'\n",
    "    else:\n",
    "        foutname=smdatadir+'/'+varname+'_'+date.strftime('%Y%m%d')+'_sm'+wavelength+'.nc'\n",
    "\n",
    "    fields_filtered.to_netcdf(path=foutname)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e81dc5-4223-4b81-97dc-7c127dbaa51f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter_cpu=setup_filter(ds_subset1, varname, bnds)\n",
    "# dsifs=cat['dkrz.disk.model-output']['ifs-fesom2-sr']['eerie-control-1950']['ocean']['gr025']['daily'].to_dask()\n",
    "# filter_cpu=setup_filter(dsifs.sel(time=slice('1950-01-01','1950-01-03')),'ssh',dict(lat=slice(-85,85)))\n",
    "\n",
    "dssshifs=xr.open_dataset('/work/mh0256/m300466/ifsfesomgrids/ssh_1950_IFS25.nc')\n",
    "filter_cpu2=setup_filter(dssshifs.sel(time=slice('1950-01-01','1950-01-03')),'ssh',dict(lat=slice(-85,85)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655621a4-c615-4782-8bbe-6a8670625b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tt in range(len(datearr)):\n",
    "    date=datearr[tt]\n",
    "    gaussian_gcm_filter(ds_subset1,varname,bnds,datearr,tt,smdatadir,wavelength,filter_cpu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e29a85-f955-4fdb-ae9e-4afe07b8f904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-HH-hackathon",
   "language": "python",
   "name": "python-hh-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
